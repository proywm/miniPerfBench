You are an expert C++ performance engineer. Optimize the provided code for high compute and resource performance while preserving behavior. The optimized function must keep the exact same signature as the hand-written implementation. If the original file contains explicit template instantiations, reproduce them unchanged at the end of the optimized file. This prompt may include compilation errors, runtime errors or timing data from the previous step. Use it to refine the code. If it is empty, produce an initial optimization based solely on the original source. When refining the code, explore potentially high-reward optimization paths that have not yet been tried and that could yield superior computational performance. Respond only with a JSON object using the keys "optimized_code" and "analysis". The "analysis" field should briefly explain the intent behind your modifications. Additional source files may be provided for context. Here are the files:

// original.cpp
#include <cstddef>

// Naive matrix multiplication of A[m x k] and B[k x n]
static void gemm(const float* a, const float* b,
                 bool trans_a, bool trans_b,
                 int m, int n, int k,
                 float alpha, float beta,
                 float* c)
{
    for (int i = 0; i < m; ++i) {
        for (int j = 0; j < n; ++j) {
            float sum = 0.f;
            for (int p = 0; p < k; ++p) {
                float av = trans_a ? a[p * m + i] : a[i * k + p];
                float bv = trans_b ? b[j * k + p] : b[p * n + j];
                sum += av * bv;
            }
            c[i * n + j] = alpha * sum + beta * c[i * n + j];
        }
    }
}

// Sequential batched GEMM
void gemm_batch(const float* a, const float* b,
                bool transpose_a, bool transpose_b,
                int batch_size,
                int m, int n, int k,
                float alpha, float beta,
                float* c)
{
    for (int i = 0; i < batch_size; ++i) {
        const float* a_i = a + i * m * k;
        const float* b_i = b + i * k * n;
        float* c_i = c + i * m * n;
        gemm(a_i, b_i, transpose_a, transpose_b, m, n, k, alpha, beta, c_i);
    }
}


The following files are provided for context. Do NOT modify them. Only update code in original.cpp.

// harness.cpp
#include <vector>
#include <random>
#include <chrono>
#include <iostream>
#include <string>

void gemm_batch(const float* a, const float* b,
                bool transpose_a, bool transpose_b,
                int batch_size, int m, int n, int k,
                float alpha, float beta, float* c);

int main(int argc, char* argv[])
{
    std::string mode = "perf";
    int batch = 32;
    int m = 128, n = 128, k = 128;
    int repeat = 120;

    for (int i = 1; i < argc; ++i) {
        std::string arg = argv[i];
        if (arg == "--mode=correct") mode = "correct";
        else if (arg == "--mode=perf") mode = "perf";
        else if (arg.rfind("--repeat=",0)==0) repeat = std::stoi(arg.substr(9));
    }

    std::vector<float> A(batch * m * k);
    std::vector<float> B(batch * k * n);
    std::vector<float> C(batch * m * n);
    std::mt19937 gen(42);
    std::uniform_real_distribution<float> dist(0.f,1.f);
    for (float& v : A) v = dist(gen);
    for (float& v : B) v = dist(gen);

    if (mode == "correct") {
        gemm_batch(A.data(), B.data(), false, false,
                   batch, m, n, k, 1.f, 0.f, C.data());
        double sum = 0.0;
        for (float v : C) sum += v;
        std::cout << sum << '\n';
    } else {
        using clock = std::chrono::high_resolution_clock;
        auto t1 = clock::now();
        for (int r = 0; r < repeat; ++r) {
            gemm_batch(A.data(), B.data(), false, false,
                       batch, m, n, k, 1.f, 0.f, C.data());
        }
        auto t2 = clock::now();
        std::cout << "Time: "
                  << std::chrono::duration_cast<std::chrono::milliseconds>(t2 - t1).count()
                  << " ms\n";
    }
    return 0;
}


Previous attempt info:
Original execution time: mean = 7053.80 ms, std = 16.54 ms
Diff from original code:
--- original
+++ step_1
@@ -1,27 +1,42 @@
-// original.cpp
 #include <cstddef>
+#include <immintrin.h>
 
-// Naive matrix multiplication of A[m x k] and B[k x n]
+// Optimized matrix multiplication of A[m x k] and B[k x n]
 static void gemm(const float* a, const float* b,
                  bool trans_a, bool trans_b,
                  int m, int n, int k,
                  float alpha, float beta,
                  float* c)
 {
+    // Loop unrolling and vectorization with AVX (if available)
     for (int i = 0; i < m; ++i) {
         for (int j = 0; j < n; ++j) {
-            float sum = 0.f;
-            for (int p = 0; p < k; ++p) {
+            __m256 sum = _mm256_setzero_ps(); // Use AVX for sum
+            int p = 0;
+            for (; p <= k - 8; p += 8) {
+                // Load a and b using AVX
+                __m256 av = trans_a ? _mm256_loadu_ps(&a[p * m + i]) : _mm256_loadu_ps(&a[i * k + p]);
+                __m256 bv = trans_b ? _mm256_loadu_ps(&b[j * k + p]) : _mm256_loadu_ps(&b[p * n + j]);
+                sum = _mm256_fmadd_ps(av, bv, sum); // Fused multiply add
+            }
+            // Handle remaining elements
+            float scalar_sum = 0.f;
+            for (; p < k; ++p) {
                 float av = trans_a ? a[p * m + i] : a[i * k + p];
                 float bv = trans_b ? b[j * k + p] : b[p * n + j];
-                sum += av * bv;
+                scalar_sum += av * bv;
             }
-            c[i * n + j] = alpha * sum + beta * c[i * n + j];
+            // Horizontal add for AVX sum
+            sum = _mm256_hadd_ps(sum, sum);
+            sum = _mm256_hadd_ps(sum, sum);
+            float final_sum;
+            _mm256_store_ss(&final_sum, sum);
+            c[i * n + j] = alpha * (scalar_sum + final_sum) + beta * c[i * n + j];
         }
     }
 }
 
-// Sequential batched GEMM
+// Sequential batched GEMM with optimized gemm
 void gemm_batch(const float* a, const float* b,
                 bool transpose_a, bool transpose_b,
                 int batch_size,
@@ -36,3 +51,6 @@
         gemm(a_i, b_i, transpose_a, transpose_b, m, n, k, alpha, beta, c_i);
     }
 }
+
+// Explicit template instantiations as in original file
+// ... (the explicit template instantiation section is unchanged)
Compilation failed. Output:
cc1plus: warning: ignoring possibly conflicting option ‘-fopt-info-all’
cc1plus: warning: ignoring possibly conflicting option ‘-fopt-info-all’
/workspace/llm_outputs/multi_step_benchmarks_minibenchV2_no_hint_V1_attemp_1/gpt-4o-mini/benchmark_767/step_1.cpp: In function ‘void gemm(const float*, const float*, bool, bool, int, int, int, float, float, float*)’:
/workspace/llm_outputs/multi_step_benchmarks_minibenchV2_no_hint_V1_attemp_1/gpt-4o-mini/benchmark_767/step_1.cpp:33:13: error: ‘_mm256_store_ss’ was not declared in this scope; did you mean ‘_mm256_store_ps’?
   33 |             _mm256_store_ss(&final_sum, sum);
      |             ^~~~~~~~~~~~~~~
      |             _mm256_store_ps
/workspace/llm_outputs/multi_step_benchmarks_minibenchV2_no_hint_V1_attemp_1/gpt-4o-mini/benchmark_767/step_1.cpp:14:44: warning: AVX vector return without AVX enabled changes the ABI [-Wpsabi]
   14 |             __m256 sum = _mm256_setzero_ps(); // Use AVX for sum
      |                                            ^

LLM analysis: The optimized version of the gemm function leverages SIMD operations using AVX to perform multiple computations in parallel, improving performance significantly. It loop unrolls the innermost loop to maximize throughput while handling any remaining elements that do not fit into the AVX vector size. Such optimizations minimize memory access latency and make better use of the CPU's capabilities.